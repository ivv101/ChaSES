{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d70badff-bbff-4181-a7f6-37d9e8f5e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import alphashape\n",
    "\n",
    "from scipy.stats import skellam\n",
    "from scipy.special import ndtri\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from astropy.wcs import WCS\n",
    "from astropy.stats import sigma_clipped_stats \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# from cuml.cluster import DBSCAN\n",
    "# from cuml.metrics.cluster import silhouette_samples\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.cluster import silhouette_samples\n",
    "\n",
    "# from sklearn.cluster import DBSCAN, OPTICS #, KMeans, SpectralClustering\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import extended_library as ext_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d4e503-7200-4330-a192-e4f1274f3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_sort(db, n_min=1):\n",
    "    \n",
    "    '''\n",
    "    delete clusters < n_min; clusters ordered so that 0 - largest cluster\n",
    "    '''\n",
    "    \n",
    "    n_clusters = np.max(db) + 1    \n",
    "    if n_clusters == 0:\n",
    "        return db\n",
    "\n",
    "    a = list(Counter(db).items())\n",
    "    a = np.array([_ for _ in a if _[0]!=-1])\n",
    "    a = a[a[:, 1].argsort()][::-1]\n",
    "    small = [_[0] for _ in a if _[1]<n_min]\n",
    "    a = dict(zip(a[:, 0], np.arange(len(a))))\n",
    "    for i in small:\n",
    "        a[i] = -1\n",
    "    a[-1] = -1\n",
    "\n",
    "    db = np.array([a[i] for i in db])\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a17c9fe-460e-46b1-a7b3-1fbc2c6a7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(obsid, ccd, fits_dir='', holes=True):\n",
    "    \n",
    "    hls = '_holes' if holes else ''\n",
    "        \n",
    "    evt2_data, head = ext_lib.process_fits(f'{fits_dir}/{obsid}/{ccd}/{obsid}_{ccd}{hls}_evt2_05_8keV.fits')\n",
    "\n",
    "    if len(evt2_data)==0:\n",
    "        print(f'{obsid}_{ccd} empty')\n",
    "        # msg.text = f'{obsid}_{ccd} empty'\n",
    "        return 'empty'\n",
    "\n",
    "    xy = ext_lib.xy_filter_evt2(evt2_data)[f'ccd_{ccd}']\n",
    "\n",
    "    scaled_xy = ext_lib.scale(*xy.T)\n",
    "    \n",
    "    scaled_xy['head'] = head\n",
    "    \n",
    "    return scaled_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb57aeb-cc0c-4be1-9808-b18e56131d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hull(clusters, alpha):\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    areas = []\n",
    "\n",
    "    for c in clusters:\n",
    "\n",
    "        # edges, hull_vertices = cxo_lib.concave_hull(c, 0.2)\n",
    "        # pgon = Polygon(zip(*hull_vertices)) \n",
    "        # dens = 100 * (len(c) / (pgon.area * len(X)) - 1)\n",
    "        \n",
    "        ashape = alphashape.alphashape(c, alpha)\n",
    "        \n",
    "        x, y = ashape.exterior.xy\n",
    "        \n",
    "        area = ashape.area\n",
    "\n",
    "        # x, y = cxo_lib.concave_hull(c, alpha)[1]\n",
    "\n",
    "        xs.append([[x.tolist()]])\n",
    "        ys.append([[y.tolist()]])\n",
    "        areas.append(area)\n",
    "\n",
    "    return {'xs': xs, 'ys': ys, 'area': areas}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139eca08-14d9-4601-b88e-e8d883235804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbins_sigma_func(X, nbins, sigma):\n",
    "\n",
    "    # k = scipy.stats.gaussian_kde([x,y], sigma)\n",
    "    # xi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\n",
    "    # zi = k(np.vstack([xi.flatten(), yi.flatten()]))    \n",
    "    # d = zi.reshape(xi.shape).T\n",
    "\n",
    "    H, xe, ye = np.histogram2d(*X.T, bins=nbins)  \n",
    "\n",
    "    H = gaussian_filter(H, sigma=sigma)            \n",
    "\n",
    "    mean, median, std = sigma_clipped_stats(H, sigma=3.0)\n",
    "    # print((mean, median, std), bkg_dens) \n",
    "\n",
    "    bkg_dens = median * nbins**2\n",
    "\n",
    "    # print('len_X:', len(X_source.data['X']))\n",
    "    # print('bkg_dens:', bkg_dens)\n",
    "\n",
    "    return H, bkg_dens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebbb32-fcb6-483d-99ed-325d5e101691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ccd(obsid, ccd, holes=True, n_lim=True, n_max='all', \n",
    "                args_func={}, nbins=100, sigma=3, alpha=1, fits_dir=''):\n",
    "\n",
    "    scaled_xy = get_data(obsid, ccd, fits_dir, holes)\n",
    "    \n",
    "    if scaled_xy=='empty':\n",
    "        return 'empty'\n",
    "    \n",
    "    # import pickle as pkl\n",
    "    # pkl.dump(scaled_xy, open('/home/ivv101/oyk/Extended_sources/2022/app_cache/scaled_xy.pkl', 'wb'))\n",
    "\n",
    "    X = scaled_xy['X'].copy()\n",
    "\n",
    "    if len(X)==0:\n",
    "        return 'empty'\n",
    "\n",
    "    len_X_orig = len(X)\n",
    "\n",
    "    if n_lim and n_max!='all':\n",
    "        np.random.shuffle(X)\n",
    "        X = X[:np.min([n_max, len_X_orig])]\n",
    "\n",
    "    args = args_func.copy()    \n",
    "\n",
    "    if 'eps' in args:\n",
    "        eps0 = float(1 / np.sqrt(len(X)))                       \n",
    "        args['eps'] *= eps0\n",
    "\n",
    "    db = DBSCAN(**args).fit_predict(X)                \n",
    "    db = db_sort(db, n_min=4)\n",
    "\n",
    "    n_clusters = db.max() + 1\n",
    "\n",
    "    noise = X[db==-1]\n",
    "    clusters = [X[db==_].tolist() for _ in range(n_clusters)] \n",
    "\n",
    "    # cp.cuda.stream.get_current_stream().synchronize()\n",
    "\n",
    "    # hulls, center of mass, area, silhouette, #cluster, n-n_bkg/area, significance \n",
    "\n",
    "    data = {}\n",
    "\n",
    "    xs_ys_areas = get_hull(clusters, alpha)        \n",
    "    data.update(xs_ys_areas)\n",
    "        \n",
    "    try:\n",
    "        silhs = silhouette_samples(X, db)   \n",
    "        \n",
    "        if silhouette_samples.__module__.split('.')[0] == 'cuml':\n",
    "        \n",
    "            silhs = [np.mean(silhs[db==_]).get().tolist() for _ in range(n_clusters)]  \n",
    "        else:\n",
    "            silhs = [np.mean(silhs[db==_]).tolist() for _ in range(n_clusters)]  \n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'no silhs for {obsid}/{ccd}')\n",
    "        silhs = [-1]*n_clusters\n",
    "        \n",
    "    data['silhouette'] = silhs    \n",
    "\n",
    "    H, bkg_dens = nbins_sigma_func(X, nbins, sigma)\n",
    "\n",
    "    com = np.transpose([np.mean(c, 0).tolist() for c in clusters])  \n",
    "    data['x_scaled'], data['y_scaled'] = com\n",
    "    \n",
    "    com = ext_lib.unscale(*com, scaled_xy['pars'])\n",
    "\n",
    "    data['x'], data['y'] = com\n",
    "\n",
    "    data['n-n_bkg'] = [len(c) - bkg_dens * a for c, a in zip(clusters, data['area'])]\n",
    "\n",
    "    data['signif.'] = [1 - skellam.cdf(x, len(X) * a, bkg_dens * a) for x, a in zip(data['n-n_bkg'], data['area'])]\n",
    "\n",
    "    data['sigmas'] = [ndtri(1-_/2) for _ in data['signif.']]\n",
    "\n",
    "    h = scaled_xy['head']\n",
    "\n",
    "    w = WCS(naxis=2)\n",
    "    w.wcs.crpix = [h['TCRPX11'], h['TCRPX12']]\n",
    "    w.wcs.cdelt = [h['TCDLT11'], h['TCDLT12']]\n",
    "    w.wcs.crval = [h['TCRVL11'], h['TCRVL12']]\n",
    "    w.wcs.ctype = [h['TCTYP11'], h['TCTYP12']]\n",
    "    w.wcs.cunit = [h['TCUNI11'], h['TCUNI12']]\n",
    "    w.wcs.radesys = 'ICRS'\n",
    "    w.wcs.mjdobs = h['MJD-OBS']\n",
    "    w.wcs.dateobs = h['DATE-OBS']\n",
    "\n",
    "    data['ra'], data['dec'] = w.wcs_pix2world(com.T, 1).T\n",
    "\n",
    "    data.update(dict(zip(['X', 'len_X_orig', 'db', 'n_clusters', 'bkg_dens', 'clusters', 'H'], \n",
    "                         [X, len_X_orig, db, n_clusters, bkg_dens, clusters, H])))\n",
    "\n",
    "    return data # no filtering by sigma_min"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rapids]",
   "language": "python",
   "name": "conda-env-rapids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
