{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d70badff-bbff-4181-a7f6-37d9e8f5e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import alphashape\n",
    "\n",
    "from scipy.stats import skellam\n",
    "from scipy.special import ndtri\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from astropy.wcs import WCS\n",
    "from astropy.stats import sigma_clipped_stats \n",
    "from astropy.io import fits\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# from cuml.cluster import DBSCAN\n",
    "# from cuml.metrics.cluster import silhouette_samples\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.cluster import silhouette_samples\n",
    "\n",
    "# from sklearn.cluster import DBSCAN, OPTICS #, KMeans, SpectralClustering\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import extended_library as ext_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26d4e503-7200-4330-a192-e4f1274f3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_sort(db, n_min=1):\n",
    "    \n",
    "    '''\n",
    "    delete clusters < n_min; clusters ordered so that 0 - largest cluster\n",
    "    '''\n",
    "    \n",
    "    n_clusters = np.max(db) + 1    \n",
    "    if n_clusters == 0:\n",
    "        return db\n",
    "\n",
    "    a = list(Counter(db).items())\n",
    "    a = np.array([_ for _ in a if _[0]!=-1])\n",
    "    a = a[a[:, 1].argsort()][::-1]\n",
    "    small = [_[0] for _ in a if _[1]<n_min]\n",
    "    a = dict(zip(a[:, 0], np.arange(len(a))))\n",
    "    for i in small:\n",
    "        a[i] = -1\n",
    "    a[-1] = -1\n",
    "\n",
    "    db = np.array([a[i] for i in db])\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a17c9fe-460e-46b1-a7b3-1fbc2c6a7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(evt2_fn, ccd):\n",
    "        \n",
    "    evt2_data, head = ext_lib.process_fits(evt2_fn)\n",
    "\n",
    "    if len(evt2_data)==0:\n",
    "        # print(f'{obsid}_{ccd} empty')\n",
    "        # msg.text = f'{obsid}_{ccd} empty'\n",
    "        return 'empty'\n",
    "\n",
    "    xy = ext_lib.xy_filter_evt2(evt2_data)[f'ccd_{ccd}']\n",
    "\n",
    "    scaled_xy = ext_lib.scale(*xy.T)\n",
    "    \n",
    "    scaled_xy['head'] = head\n",
    "    \n",
    "    return scaled_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce00854-5842-4c7c-8445-7179e6d5f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_old(obsid, ccd, fits_dir='', holes=True):\n",
    "    \n",
    "#     hls = '_holes' if holes else ''\n",
    "        \n",
    "#     evt2_data, head = ext_lib.process_fits(f'{fits_dir}/{obsid}/{ccd}/{obsid}_{ccd}{hls}_evt2_05_8keV.fits')\n",
    "\n",
    "#     if len(evt2_data)==0:\n",
    "#         print(f'{obsid}_{ccd} empty')\n",
    "#         # msg.text = f'{obsid}_{ccd} empty'\n",
    "#         return 'empty'\n",
    "\n",
    "#     xy = ext_lib.xy_filter_evt2(evt2_data)[f'ccd_{ccd}']\n",
    "\n",
    "#     scaled_xy = ext_lib.scale(*xy.T)\n",
    "    \n",
    "#     scaled_xy['head'] = head\n",
    "    \n",
    "#     return scaled_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb57aeb-cc0c-4be1-9808-b18e56131d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hull(clusters, alpha):\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    areas = []\n",
    "\n",
    "    for c in clusters:\n",
    "\n",
    "        # edges, hull_vertices = cxo_lib.concave_hull(c, 0.2)\n",
    "        # pgon = Polygon(zip(*hull_vertices)) \n",
    "        # dens = 100 * (len(c) / (pgon.area * len(X)) - 1)\n",
    "        \n",
    "        ashape = alphashape.alphashape(c, alpha)\n",
    "        \n",
    "        x, y = ashape.exterior.xy\n",
    "        \n",
    "        area = ashape.area\n",
    "\n",
    "        # x, y = cxo_lib.concave_hull(c, alpha)[1]\n",
    "\n",
    "        xs.append([[x.tolist()]])\n",
    "        ys.append([[y.tolist()]])\n",
    "        areas.append(area)\n",
    "\n",
    "    return {'xs': xs, 'ys': ys, 'area': areas}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "139eca08-14d9-4601-b88e-e8d883235804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbins_sigma_func(X, nbins, sigma):\n",
    "\n",
    "    # k = scipy.stats.gaussian_kde([x,y], sigma)\n",
    "    # xi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\n",
    "    # zi = k(np.vstack([xi.flatten(), yi.flatten()]))    \n",
    "    # d = zi.reshape(xi.shape).T\n",
    "\n",
    "    H, xe, ye = np.histogram2d(*X.T, bins=nbins)  \n",
    "\n",
    "    H = gaussian_filter(H, sigma=sigma)            \n",
    "\n",
    "    mean, median, std = sigma_clipped_stats(H, sigma=3.0)\n",
    "    # print((mean, median, std), bkg_dens) \n",
    "\n",
    "    bkg_dens = median * nbins**2\n",
    "\n",
    "    # print('len_X:', len(X_source.data['X']))\n",
    "    # print('bkg_dens:', bkg_dens)\n",
    "\n",
    "    return H, bkg_dens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bf1379-3098-4a3b-baa4-48a0428a88c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obsid = 88\n",
    "# ccd = 0\n",
    "\n",
    "# loc = 'hug'\n",
    "\n",
    "# holes = True\n",
    "\n",
    "# fff = {'local': '/home/ivv101/oyk/Extended_sources/2022/Chandra-ACIS-clusters-app/data',\n",
    "#  'hug': 'https://huggingface.co/datasets/oyk100/Chandra-ACIS-clusters-data/resolve/main'}\n",
    "\n",
    "# local_fits_dir = fff['local']\n",
    "\n",
    "# fits_dir = fff[loc]\n",
    "\n",
    "# hls = '_holes' if holes else ''\n",
    "\n",
    "# args_func = {\n",
    "#             'eps': 2.7, \n",
    "#             'min_samples': 46\n",
    "#         }\n",
    "\n",
    "# dat = process_ccd(obsid, ccd, holes=True, n_lim=True, n_max='all', \n",
    "#                 args_func=args_func, nbins=100, sigma=3, alpha=1, fits_dir=fits_dir, local_fits_dir=local_fits_dir, loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b7cfc-a281-4117-9283-c35720cb71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fits(obsid, ccd, fits_dir, hls, cache=False):\n",
    "    \n",
    "    obsid_dir = f'{fits_dir}/{obsid}'\n",
    "    fn_ccd = f'{obsid_dir}/{ccd}/{obsid}_{ccd}{hls}_evt2_05_8keV.fits'\n",
    "        \n",
    "    if cache and os.path.isfile(fn_ccd):\n",
    "        return\n",
    "    \n",
    "    fn = glob(f'{obsid_dir}/*fits*')[0]\n",
    "\n",
    "    with fits.open(fn) as hdul:\n",
    "\n",
    "        # hdul.info()\n",
    "        X = hdul[1].data\n",
    "        head = hdul[1].header\n",
    "    \n",
    "    cols = ['ccd_id', 'x', 'y', 'energy']\n",
    "    ccds = np.sort(np.unique(X['ccd_id'])).tolist()\n",
    "\n",
    "    mask = (500 < X['energy']) & (X['energy'] < 8000)\n",
    "\n",
    "    X = np.array(X)[mask][cols]\n",
    "\n",
    "    X = X[X['ccd_id']==int(ccd)]\n",
    "    \n",
    "    bt = fits.BinTableHDU(X, head)\n",
    "    bt.name = 'EVENTS'\n",
    "    \n",
    "    bt.writeto(fn_ccd, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53ebbb32-fcb6-483d-99ed-325d5e101691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ccd(obsid, ccd, holes=True, n_lim=True, n_max='all', \n",
    "                args_func={}, nbins=100, sigma=3, alpha=1, local_fits_dir=''):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        holes=True not implemented for query or local custom (ciao...)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    hls = '_holes' if holes else ''        \n",
    "    evt2_fn = f'{obsid}_{ccd}{hls}_evt2_05_8keV.fits'\n",
    "    \n",
    "    evt2_fn_local = f'{local_fits_dir}/{obsid}/{ccd}/{evt2_fn}'\n",
    "    \n",
    "#     if os.path.isfile(evt2_fn_local):\n",
    "#         print('pass')\n",
    "#         pass \n",
    "    \n",
    "#     elif loc=='hug':        \n",
    "#         url = f'{fits_dir}/{obsid}/{ccd}/{evt2_fn}'    \n",
    "#         # print('url')\n",
    "#         os.system(f'mkdir -p {local_fits_dir}/{obsid}/{ccd}')\n",
    "#         # print('done mkdir')\n",
    "#         urlretrieve(url, evt2_fn_local)   \n",
    "        \n",
    "#     elif loc=='local':\n",
    "        \n",
    "#         evt2_fn = f'{obsid}_{ccd}_evt2_05_8keV.fits' # no holes\n",
    "#         evt2_fn_local = f'{local_fits_dir}/{obsid}/{ccd}/{evt2_fn}'\n",
    "        \n",
    "#         if not os.path.isfile(evt2_fn_local):\n",
    "            \n",
    "#             create_fits(obsid, ccd, local_fits_dir, cache=True) \n",
    "        \n",
    "    # elif loc=='query':        \n",
    "    #     status, url, evt2_fn_local = ext_lib.get_evt2_file(obsid, f'{local_fits_dir}/{obsid}')        \n",
    "    #     if status != 'ok':\n",
    "    #         sys.exit(status)   \n",
    "            \n",
    "    scaled_xy = get_data(evt2_fn_local, ccd)    \n",
    "    \n",
    "    # scaled_xy = get_scaled_xy(obsid, ccd, holes=holes, fits_dir=fits_dir, local_fits_dir=local_fits_dir, loc=loc)    \n",
    "\n",
    "    # scaled_xy = get_data_old(obsid, ccd, fits_dir=fits_dir, holes=holes)\n",
    "    \n",
    "    if scaled_xy=='empty':\n",
    "        return 'empty'\n",
    "    \n",
    "    # import pickle as pkl\n",
    "    # pkl.dump(scaled_xy, open('/home/ivv101/oyk/Extended_sources/2022/app_cache/scaled_xy.pkl', 'wb'))\n",
    "\n",
    "    X = scaled_xy['X'].copy()\n",
    "\n",
    "    if len(X)==0:\n",
    "        return 'empty'\n",
    "\n",
    "    len_X_orig = len(X)\n",
    "\n",
    "    if n_lim and n_max!='all':\n",
    "        np.random.shuffle(X)\n",
    "        X = X[:np.min([n_max, len_X_orig])]\n",
    "\n",
    "    args = args_func.copy()    \n",
    "\n",
    "    if 'eps' in args:\n",
    "        eps0 = float(1 / np.sqrt(len(X)))                       \n",
    "        args['eps'] *= eps0\n",
    "\n",
    "    db = DBSCAN(**args).fit_predict(X)                \n",
    "    db = db_sort(db, n_min=4)\n",
    "\n",
    "    n_clusters = db.max() + 1\n",
    "\n",
    "    noise = X[db==-1]\n",
    "    clusters = [X[db==_].tolist() for _ in range(n_clusters)] \n",
    "\n",
    "    # cp.cuda.stream.get_current_stream().synchronize()\n",
    "\n",
    "    # hulls, center of mass, area, silhouette, #cluster, n-n_bkg/area, significance \n",
    "\n",
    "    data = {}\n",
    "\n",
    "    xs_ys_areas = get_hull(clusters, alpha)        \n",
    "    data.update(xs_ys_areas)\n",
    "        \n",
    "    try:\n",
    "        silhs = silhouette_samples(X, db)   \n",
    "        \n",
    "        if silhouette_samples.__module__.split('.')[0] == 'cuml':\n",
    "        \n",
    "            silhs = [np.mean(silhs[db==_]).get().tolist() for _ in range(n_clusters)]  \n",
    "        else:\n",
    "            silhs = [np.mean(silhs[db==_]).tolist() for _ in range(n_clusters)]  \n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'no silhs for {obsid}/{ccd}')\n",
    "        silhs = [-1]*n_clusters\n",
    "        \n",
    "    data['silhouette'] = silhs    \n",
    "\n",
    "    H, bkg_dens = nbins_sigma_func(X, nbins, sigma)\n",
    "\n",
    "    com = np.transpose([np.mean(c, 0).tolist() for c in clusters])  \n",
    "    data['x_scaled'], data['y_scaled'] = com\n",
    "    \n",
    "    com = ext_lib.unscale(*com, scaled_xy['pars'])\n",
    "\n",
    "    data['x'], data['y'] = com\n",
    "\n",
    "    data['n-n_bkg'] = [len(c) - bkg_dens * a for c, a in zip(clusters, data['area'])]\n",
    "\n",
    "    data['signif.'] = [1 - skellam.cdf(x, len(X) * a, bkg_dens * a) for x, a in zip(data['n-n_bkg'], data['area'])]\n",
    "\n",
    "    data['sigmas'] = [ndtri(1-_/2) for _ in data['signif.']]\n",
    "\n",
    "    h = scaled_xy['head']\n",
    "\n",
    "    w = WCS(naxis=2)\n",
    "    w.wcs.crpix = [h['TCRPX11'], h['TCRPX12']]\n",
    "    w.wcs.cdelt = [h['TCDLT11'], h['TCDLT12']]\n",
    "    w.wcs.crval = [h['TCRVL11'], h['TCRVL12']]\n",
    "    w.wcs.ctype = [h['TCTYP11'], h['TCTYP12']]\n",
    "    w.wcs.cunit = [h['TCUNI11'], h['TCUNI12']]\n",
    "    w.wcs.radesys = 'ICRS'\n",
    "    try:\n",
    "        w.wcs.mjdobs = h['MJD-OBS']\n",
    "    except:\n",
    "        w.wcs.mjdobs = h['MJD_OBS']\n",
    "    w.wcs.dateobs = h['DATE-OBS']\n",
    "\n",
    "    data['ra'], data['dec'] = w.wcs_pix2world(com.T, 1).T\n",
    "\n",
    "    data.update(dict(zip(['X', 'len_X_orig', 'db', 'n_clusters', 'bkg_dens', 'clusters', 'H'], \n",
    "                         [X, len_X_orig, db, n_clusters, bkg_dens, clusters, H])))\n",
    "\n",
    "    return data # no filtering by sigma_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e47909-6fdf-43a1-8b80-0dd17e03eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loop_class:\n",
    "    \n",
    "    sp = ''.join([' ']*100)\n",
    "        \n",
    "    def __init__(self, lst):\n",
    "\n",
    "        self.t0 = timer()\n",
    "        self.n = len(lst) \n",
    "        self.tt = []\n",
    "                \n",
    "    def __call__(self):\n",
    "        t = timer()\n",
    "        self.tt.append(t)\n",
    "        \n",
    "        k = len(self.tt)\n",
    "        \n",
    "        perc = 100 * k / self.n\n",
    "    \n",
    "        rem = int((self.n - k) * (t - self.t0) / k)\n",
    "        \n",
    "        print(f'\\r{self.sp}', end='')\n",
    "\n",
    "        if k < self.n:    \n",
    "            msg = f'\\r{k}/{self.n}: {perc:.1f}%, {timedelta(seconds=rem)} remaining'    \n",
    "        else:\n",
    "            msg = f'\\r{self.n} done, {timedelta(seconds=int(t - self.t0))} total'\n",
    "\n",
    "        print(msg, end='', flush=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145b7187-43a3-4753-a5dc-e932fca08383",
   "metadata": {},
   "outputs": [],
   "source": [
    "class friz_class:\n",
    "    \n",
    "    def __init__(self, history=False, inactive=False): \n",
    "                        \n",
    "        self.data = {}        \n",
    "        self.pref = '' if not inactive else 'INACTIVE'\n",
    "        \n",
    "        self.comment = ''\n",
    "        \n",
    "        self.inactive = inactive\n",
    "                \n",
    "        if history:        \n",
    "            self.history = []\n",
    "        \n",
    "    def freeze(self, model):\n",
    "        self.data[model] = True if not self.inactive else False\n",
    "        self.history.append(f'{model} freeze {self.pref}')\n",
    "        \n",
    "    def unfreeze(self, model):\n",
    "        \n",
    "        if model in self.data and self.data[model]==True:\n",
    "            self.data[model] = False\n",
    "            self.history.append(f'{model} unfreeze {self.pref}')\n",
    "            return True\n",
    "        else:\n",
    "            self.history.append(f'{model} passed {self.pref}')\n",
    "            return False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72649336-70a6-4630-9b2e-28897a6f5720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rapids]",
   "language": "python",
   "name": "conda-env-rapids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
